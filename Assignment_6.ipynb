{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade: /100 pts\n",
    "\n",
    "# Assignment 06: Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Preprocessing (5 pts)\n",
    "**Make sure to only use the imports included in the first cell. Any additional imports used will result in 0.** <br>\n",
    "Tasks:\n",
    "* Load the data present in 'footballer_small.csv' using the pandas library and store the loaded data in a dataframe\n",
    "* Drop the variables: 'ID','club','club_logo','flag', 'nationality','photo','potential', 'birth_date'\n",
    "* Dummy code the variables: work_rate_att, work_rate_def, preferred_foot. **Because we are running a regularized model, we do not want to drop the first column**\n",
    "* Get a test data set of size 500 - to make results comparable to solutions, set random_state = 0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data, dropping unnecessary variables, dummy coding categorical variables.\n",
    "model_data = pd.read_csv(\"footballer_small.csv\")\n",
    "model_data = model_data.drop(columns=['ID', 'club', 'club_logo', 'flag', 'nationality', 'photo', 'potential', 'birth_date'])\n",
    "model_data = pd.get_dummies(model_data, drop_first=False)\n",
    "\n",
    "# Getting predictor and outcome variables, splitting into train and test.\n",
    "X = model_data.drop(columns=\"overall\")\n",
    "y = model_data.overall\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y, test_size = 500, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Building the pipeline that preprocesses the data (10 pts)\n",
    "In order to properly build the regression model, the features need to be standardized so that no feature can dominate others in determining the prediction values due to differences in feature scales. Build the pipeline that preprocesses the feature columns of the training data and create a linear regression model. Plot the data before and after the standardization for the stamina feature. <br>\n",
    "In this question, there is no need to overwrite the training set's values. Create a new variable to include the standardized data. The original training data is needed for future exercises. <br>\n",
    "### Question\n",
    "Make observations about the plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Written answer: the plots appear to have the same distribution shape (minus some difference because of different bins for each plot), however the standardized plot is centred around 0 with a stdev of 1, whereas the unstandardized one is centred around the mean of stamina with a stdev of stamina's stdev.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a class for the standardizer transformer.\n",
    "class Standardizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Getting categorical variables, getting list of all non-categorical variables in this particular instance.\n",
    "        # Note this is generalizable to inputs missing certain variables (question 4)\n",
    "        categorical_variables = ['work_rate_att','work_rate_def','preferred_foot']\n",
    "        model_continuous_vars = [column for column in X.columns if all(cat_var not in column for cat_var in categorical_variables)]\n",
    "        \n",
    "        # Standardizing all non-categorical variables, returning transformed data frame.\n",
    "        standardization_steps=[('scaler', StandardScaler(), model_continuous_vars)]\n",
    "        ct = ColumnTransformer(standardization_steps, remainder='passthrough')\n",
    "        X = ct.fit_transform(X)\n",
    "        return X\n",
    "\n",
    "# Creating a column transformer to standardize non-categorical variables.\n",
    "standardization_steps=[('scaler', Standardizer(), X.columns.tolist())]\n",
    "ct = ColumnTransformer(standardization_steps)\n",
    "standardized = ct.fit_transform(Xtrain)\n",
    "\n",
    "# Turning standardized output into a data frame so that its columns are accessible.\n",
    "standardized = pd.DataFrame(standardized, columns = X.columns.tolist())\n",
    "\n",
    "# Creating a pipeline that standardizes non-categorical variables and runs a linear regression.\n",
    "steps = [('scaler', ct), ('linear_regression', LinearRegression())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "# Plotting distributions of standardized & non-standardized stamina values\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.hist(X.stamina, alpha=0.5, label='unstandardized', color='black')\n",
    "ax1.legend()\n",
    "ax2.hist(standardized.stamina, alpha=0.5, label='standardized')\n",
    "ax2.legend()\n",
    "\n",
    "'''Written answer: the plots appear to have the same distribution shape (minus some difference because of different bins for each plot), however the standardized plot is centred around 0 with a stdev of 1, whereas the unstandardized one is centred around the mean of stamina with a stdev of stamina\\'s stdev.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Creating a Complex feature model using 2-nd Polynomial features (10 pts)\n",
    "In this task, the training data consists of the quadratic terms and 2-way interactions of all features except one of standing_tackle, composure, and marking variable. <br> \n",
    "Create <b> four </b> different training sets:\n",
    "* The first training set includes the quadratic terms and 2-way interactions of all the features. \n",
    "* The second/third/fourth training sets include all the features, their quadratic terms, and their 2-way interactions except the standing_tackle/composure/marking features. <br>\n",
    "\n",
    "#### Hint: \n",
    "For the 2nd/3rd and 4th training sets, create the training sets without the aforementioned features and then apply polynomial expansion to the resultant sets. \n",
    "### Questions:\n",
    "* How many linear terms are in each of the new feature set?\n",
    "        In the feature set including all variables, there are 48 linear features--one for each feature.\n",
    "        In the feature sets missing one variable, there are 47 linear variables for the same reason.\n",
    "* How many squared terms are in each of the new feature set?\n",
    "        In the feature set including all variables, there are 48 quadratic features--one for each feature.\n",
    "        In the feature sets missing one variable, there are 47 linear variables for the same reason.\n",
    "* How many interaction terms are in each of the new feature set? Give an example of one of the interaction terms. \n",
    "        There are nC2=n(n-1)/2 interaction terms in each feature set, given that there is an interaction set for each possible pair. For the set including all variables that is 48(47)/2 = 1128. \n",
    "        For the sets missing one variable each, that is 47(46)/2 = 1081."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1224)\n",
      "(2500, 1175)\n",
      "(2500, 1175)\n",
      "(2500, 1175)\n"
     ]
    }
   ],
   "source": [
    "# Creating a polynomial expander.\n",
    "PT = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "\n",
    "# Getting column names for each of the outputs\n",
    "cols = [None, 'standing_tackle', 'composure', 'marking']\n",
    "quad_train = []\n",
    "n_sets = 4\n",
    "\n",
    "for i in range(n_sets):\n",
    "    dropped = cols[i]\n",
    "    # Dropping column if necessary, creating data frame with polynomially expanded values and column titles.\n",
    "    # Note: These are unstandardized values, as the question did not ask for standardized values.\n",
    "    if dropped == None:\n",
    "        feature_names = PT.fit(Xtrain).get_feature_names(Xtrain.columns.tolist())\n",
    "        quad_train.append(pd.DataFrame(PT.fit_transform(Xtrain), columns=feature_names))\n",
    "    else:\n",
    "        valid_data = Xtrain.drop(columns=cols[i])\n",
    "        feature_names = PT.fit(valid_data).get_feature_names(valid_data.columns.tolist())\n",
    "        quad_train.append(pd.DataFrame(PT.fit_transform(valid_data), columns=feature_names))\n",
    "    \n",
    "for training in quad_train:\n",
    "    print(training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Evaluating the backward feature search (15 pts)\n",
    "In this question, you have to use the pipeline created in question 2 and apply it to each of the training sets created in question 3. Use 10-fold cross validation to report the validation error on the training set using mean squared error as the metric. <br>\n",
    "Show all the steps of the process and compare and analyze the results using the validation error reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.995927659800491, 3.693540317823291, 3.7473344578754904, 3.5986165091853826]\n"
     ]
    }
   ],
   "source": [
    "quad_model = []\n",
    "\n",
    "# Creating a new pipeline that polynomially expands values after standardization.\n",
    "# Note: This is a new pipeline because standardization after polynomial expansion is not possible because of column name inconsistencies.\n",
    "quad_pipe =Pipeline([('scaler', Standardizer()), ('poly', PT),('linear_regression', LinearRegression())])\n",
    "\n",
    "for i in range(n_sets):\n",
    "    # Getting the training set of values with necessary columns dropped.\n",
    "    if(i==0):\n",
    "        train = Xtrain\n",
    "    else:\n",
    "        train = Xtrain.drop(columns=cols[i])\n",
    "    quad_train[i] = train\n",
    "    \n",
    "    # Fitting the training set to the pipe that polynomially expands the standardized values. Saving the model.\n",
    "    model = quad_pipe.fit(train, ytrain)\n",
    "    quad_model.append(model)\n",
    "    \n",
    "quad_cv_score = []\n",
    "for i in range(n_sets):\n",
    "    # Calculating the CV score for each model and training set.\n",
    "    cv_score = cross_val_score(quad_model[i], quad_train[i], ytrain, cv=10, scoring='neg_mean_squared_error')\n",
    "    quad_cv_score.append(-cv_score.mean())\n",
    "print(quad_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Applying Ridge Regression (10 pts)\n",
    "Build a pipeline that performs scaling and fits the ridge regression on the data that includes the polynomial expansion of all the features. The penalization parameter is set to 0.5. Use the pipeline to report the validation error using mean square error metric. Use 10-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.785777156075923\n"
     ]
    }
   ],
   "source": [
    "# Creating a pipeline that standardizes, polynomially expands the standardized value, then applies ridge regression with alpha=0.5\n",
    "ridge_steps = [('scaler', Standardizer()), ('poly', PT), ('ridge_regression', Ridge(alpha=0.5))]\n",
    "ridge_pipe = Pipeline(ridge_steps)\n",
    "\n",
    "# Fitting the pipeline to the polynomial expansion of all the features (i.e on the complete training data). Reporting CV score\n",
    "ridge_model = ridge_pipe.fit(Xtrain, ytrain)\n",
    "ridge_cv_score = - cross_val_score(ridge_model, Xtrain, ytrain, cv=10, scoring = 'neg_mean_squared_error').mean()\n",
    "print(ridge_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Tune the Ridge coefficient for the 2nd-order model   (15 pts)\n",
    "Now use 10-fold CV on the training set to determine a good setting for the regularization coefficient. Perform the search going from $\\lambda = \\exp(-8), \\cdots, \\exp(6)$ in 15 evenly spaced increments on the log scale. Plot the mean squared error as a function of $\\log(\\lambda)$. \n",
    "\n",
    "### Questions: \n",
    "\n",
    "What is the best regularization parameter (actual not approximate)? \n",
    "* The best regularization parameter is $e^{5}$.\n",
    "\n",
    "Why does the error increase as $\\lambda \\rightarrow 0?$  Why does the error increase when $\\lambda \\rightarrow \\infty$?  Answer in terms of the bias variance trade off.\n",
    "* As $\\lambda$ approaches 0, there is no regularization and the coefficients can grow unconstrained. As a result there is a possibility these coefficients will lead to overfitting, and lead to a large variance in the data.\n",
    "* As $\\lambda$ approaches $\\infty$, all of the coefficients will approach 0, leading to a poor class of function retrieved, which is not well fit to the training data, thus possessing a high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9d67f5e610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lam = np.exp(np.linspace(-8,6,15))\n",
    "mse = np.zeros(len(lam))\n",
    "\n",
    "# Iterating through each different lambda value.\n",
    "for i in range(len(lam)):\n",
    "    # Creating a temporary pipeline that standardizes, polynomially expands, and runs a ridge regression with the current value of lambda.\n",
    "    temp_steps = [('scaler', Standardizer()), ('poly', PT), ('ridge_regression', Ridge(alpha=lam[i]))]\n",
    "    temp_pipe = Pipeline(temp_steps)\n",
    "    temp_model = temp_pipe.fit(Xtrain, ytrain)\n",
    "    \n",
    "    # Recording the scores for each lambda.\n",
    "    cv_scores = cross_val_score(temp_model, Xtrain, ytrain, cv=10, scoring = 'neg_mean_squared_error')\n",
    "    mse[i] = -cv_scores.mean()\n",
    "    \n",
    "# Outputting a scatterplot of lambda values.\n",
    "plt.scatter(np.log(lam), mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Test error  (10 pts)\n",
    "Now test how the ridge model, fitted to the whole training set, performs on the test data set. \n",
    "\n",
    "\n",
    "Report the following:\n",
    "\n",
    "* The mean squared error on the test data - along with the 95% confidence interval, determined with the central limit theorem. \n",
    "* The proportion of the variance explained by your model - along wth a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Mean: 2.057337635208312, confidence interval: (1.7179112324153898, 2.3967640380012343)\n",
      "\n",
      "R2\n",
      "Mean: 0.9557158624498421, confidence interval: (0.9543464048916879, 0.9570853200079963)\n"
     ]
    }
   ],
   "source": [
    "# Creating a bootstrap function for r2\n",
    "def r2_dist(Xtest, ytest, best_ridge_model, columns, numboot=100):\n",
    "    n = len(Xtest)\n",
    "    r2_vals = np.zeros(numboot)\n",
    "    \n",
    "    # Concatenating training and testing data into one such that the overalls correspond with their inputs. Using a data frame for easy column access/manipulation.\n",
    "    samp = pd.DataFrame(np.c_[Xtest, ytest], columns=columns)\n",
    "    \n",
    "    for i in range(numboot):\n",
    "        # Bootstrapping random samples, calculating r2.\n",
    "        d = samp.sample(n, replace=True)\n",
    "        rsq = best_ridge_model.score(d.drop(columns=\"overall\"), d.overall)\n",
    "        r2_vals[i] = rsq\n",
    "    return r2_vals\n",
    "\n",
    "\n",
    "# Get a ridge model that's fit to the training data with the best alpha.\n",
    "\n",
    "best_ridge_pipeline = Pipeline([('scaler', Standardizer()), ('poly', PT), ('ridge_regression', Ridge(alpha=np.exp(5)))])\n",
    "best_ridge_model = best_ridge_pipeline.fit(Xtrain,ytrain)\n",
    "# Predict and get the errors\n",
    "yp = best_ridge_model.predict(Xtest)\n",
    "squared_errors = (ytest - yp)**2\n",
    "\n",
    "# Calculate MSE and confidence interval distance\n",
    "mse = np.mean(squared_errors)\n",
    "interval_distance = 1.96 * np.std(squared_errors) / np.sqrt(len(squared_errors))\n",
    "\n",
    "# Bootstrapping r2 values. Getting the mean and confidence interval distance.\n",
    "r2_vals = r2_dist(Xtest, ytest, best_ridge_model, X.columns.tolist()+[\"overall\"])\n",
    "rsq = np.mean(r2_vals)\n",
    "r2sqe = np.std(r2_vals)\n",
    "\n",
    "rsq_interval_distance = 1.96 * r2sqe / np.sqrt(len(r2_vals))\n",
    "\n",
    "print(\"ERROR\\nMean: {}, confidence interval: {}\\n\".format(mse, (mse-interval_distance, mse+interval_distance)))\n",
    "print(\"R2\\nMean: {}, confidence interval: {}\".format(rsq, (rsq-rsq_interval_distance, rsq+rsq_interval_distance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Analyzing Ridge Coefficients (15 pts)\n",
    "In this question, you are first required to extract all the coefficients of the standing tackle, composure and marking features from the best model of question 7. After that, calculate the mean of the coefficients of the aforementioned features and analyze the results. Based on these observations, draw conclusions about the results in question 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('standing_tackle', 0.033449904588563915), ('composure', 0.01534353355669744), ('marking', 0.014551499170463774)]\n",
      "\n",
      "\n",
      "Written answer: because marking has the lowest coefficient here on standardized variables, it appears it  has the highest degree of collinearity. As a result, removing it and all its interactions from the regression  reduced the error the most, as a more independent set of predictors was present. However, we see that standing  tackle has a higher coefficient than composure, despite the fact that removing standing tackle improves the CV  error more. This likely means that although standing tackle has a high degree of multicollinearity, it is a more  important predictor variable on its own, so other predictors would go to zero while it remains higher.\n"
     ]
    }
   ],
   "source": [
    "extract_indices = {}\n",
    "extract_coefs = {}\n",
    "quad_columns = PT.fit(Xtrain).get_feature_names(Xtrain.columns)\n",
    "quad_coefs = best_ridge_model.named_steps['ridge_regression'].coef_\n",
    "for col in cols:\n",
    "    if col!=None:\n",
    "        indices=[]\n",
    "        coefs = []\n",
    "        for i in range(len(quad_columns)):\n",
    "            if col in quad_columns[i]:\n",
    "                indices.append(i)\n",
    "                coefs.append(quad_coefs[i])\n",
    "        extract_indices[col] = indices\n",
    "        extract_coefs[col] = coefs\n",
    "mean_coefs = [(col, np.mean(extract_coefs[col])) for col in extract_coefs.keys()]\n",
    "print(mean_coefs)\n",
    "print(\"\\n\\nWritten answer: because marking has the lowest coefficient here on standardized variables, it appears it \\\n",
    " has the highest degree of collinearity while not providing much insight on its own. As a result, removing it and all its interactions from the regression \\\n",
    " reduced the error the most, as a more independent set of predictors was present. However, we see that standing \\\n",
    " tackle has a higher coefficient than composure, despite the fact that removing standing tackle improves the CV \\\n",
    " error more. This likely means that although standing tackle has a high degree of multicollinearity, it is a more \\\n",
    " important predictor variable on its own, so other predictors would go to zero while it remains higher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: Lasso Regression (10 pts)\n",
    "Let us assume that you are building a linear regression model using only three features: standing tackle, marking, and composure to predict a player's overall rating. Based on the results of question 8, what do you think will be the features' coefficients while applying the lasso regression optimization? Answer in no more than 5 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It is likely that standing tackle coefficient would be non-zero, while the others would be 0. This is because while ridge regression looks to make all coefficients small to a certain degree, lasso regression looks to make as many coefficients as possible 0. Since standing tackle has the highest ridge coefficient of the three, that means it performs the best in spite of its multicollinearity. As a result, it is best to use only standing tackle and not the other two to reduce computational complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
